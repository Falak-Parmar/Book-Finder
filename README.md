üìö Book Finder ‚Äî Big Data Engineering Project

Overview

This project is part of the Big Data Engineering course at DA-IICT. The objective is to design and implement a robust data engineering pipeline that transforms sparse university library records into a high-quality, enriched dataset suitable for powering an ML-based semantic book search system.

The motivating use case is:

A user should be able to search for ‚Äúa story about a lonely robot in space‚Äù and retrieve relevant books, even if those exact words do not appear in the original library metadata.

This repository implements Phase 1 (Data Engineer role) of the overall project.

‚∏ª

Problem Context & Motivation

University library datasets are typically operational, not analytical:
	‚Ä¢	Titles and authors are present
	‚Ä¢	Rich metadata (ISBNs, descriptions, subjects, thumbnails) is missing
	‚Ä¢	Records are inconsistent and difficult to use for downstream ML tasks

At DA-IICT, the library accession register CSV contains only:
	‚Ä¢	Accession metadata
	‚Ä¢	Title
	‚Ä¢	Author / Editor
	‚Ä¢	Publisher, Year, Pages

This project bridges that gap by:
	1.	Treating the DA-IICT CSV as the ground-truth scope
	2.	Enriching each record using external public APIs (Google Books)
	3.	Cleaning, deduplicating, and normalizing the data
	4.	Exposing the curated dataset via a FastAPI service for future ML use

‚∏ª

High-Level Architecture

The pipeline is designed as four explicit stages, mirroring real-world data engineering systems:
	1.	Ingestion ‚Äì Collect raw data from multiple sources
	2.	Transformation ‚Äì Clean, normalize, and deduplicate
	3.	Storage ‚Äì Persist curated data in a relational database
	4.	Serving ‚Äì Expose data through stable APIs

Each stage is implemented as a separate module, enabling clarity, testability, and extensibility.

‚∏ª

Data Sources

Primary Source (Scope Anchor)
	‚Ä¢	DA-IICT Library Accession Register (CSV)
	‚Ä¢	Defines the authoritative list of books
	‚Ä¢	Ensures compliance with the DAU-only scope requirement

Secondary Source (Enrichment)
	‚Ä¢	Google Books API
	‚Ä¢	Used only to enrich existing DA-IICT titles
	‚Ä¢	Provides:
	‚Ä¢	ISBN-10 / ISBN-13
	‚Ä¢	Descriptions / blurbs
	‚Ä¢	Subjects / categories
	‚Ä¢	Thumbnails and language metadata

No new books are introduced from APIs‚Äîonly metadata augmentation is performed.

‚∏ª

Detailed Pipeline Design

1. Sync Layer (sync_pipeline.py)

Purpose:
Handle synchronization with the library‚Äôs OPAC system and manage incremental updates.

What it does:
	‚Ä¢	Crawls the Koha OPAC ‚ÄúNew Arrivals‚Äù shelf
	‚Ä¢	Dynamically identifies shelf IDs
	‚Ä¢	Downloads BibTeX records where possible

Key Challenge:
	‚Ä¢	The OPAC uses Altcha, a proof-of-work security mechanism that blocks automated scraping

Design Decision:
	‚Ä¢	Detect security blocks gracefully
	‚Ä¢	Fall back to existing records instead of corrupting the pipeline
	‚Ä¢	Log warnings rather than failing hard

This ensures the pipeline remains fault-tolerant.

‚∏ª

2. Ingestion Layer (ingestion.py)

Purpose:
Fetch rich metadata for DA-IICT books using external APIs.

Key Design Choices:
	‚Ä¢	Asynchronous requests using aiohttp
	‚Ä¢	Concurrency control via asyncio.Semaphore
	‚Ä¢	Exponential backoff with jitter to handle HTTP 429 rate limits
	‚Ä¢	Incremental ingestion using JSONL logs to avoid re-fetching data

Why async?
	‚Ä¢	The CSV contains ~26,000 records
	‚Ä¢	Synchronous API calls would be prohibitively slow and rate-limited

Output:
	‚Ä¢	Line-delimited JSON (.jsonl) files containing raw enriched metadata

‚∏ª

3. Transformation Layer (transformation.py)

Purpose:
Resolve real-world ‚Äúdirty data‚Äù issues and prepare the dataset for storage.

Operations performed:
	‚Ä¢	Unicode normalization (handling non-standard characters)
	‚Ä¢	Title and author normalization
	‚Ä¢	Removal of placeholder / empty descriptions
	‚Ä¢	Deduplication using:
	‚Ä¢	ISBN-13 (preferred)
	‚Ä¢	Google Books ID (fallback)

Conflict Resolution Strategy:
	‚Ä¢	Prefer records with valid ISBNs
	‚Ä¢	Merge metadata instead of duplicating rows

This stage converts raw API responses into a clean, analytical dataset.

‚∏ª

4. Storage Layer (storage.py, db.py)

Purpose:
Persist curated data in a structured, queryable format.

Technology:
	‚Ä¢	SQLite
	‚Ä¢	SQLAlchemy ORM

Why SQLite?
	‚Ä¢	Lightweight and portable
	‚Ä¢	No external database dependency
	‚Ä¢	Ideal for coursework and reproducibility

Schema Highlights:
	‚Ä¢	books table
	‚Ä¢	Indexed columns on isbn_13 and title
	‚Ä¢	Enforced schema integrity

‚∏ª

5. Serving Layer (serving.py)

Purpose:
Expose the curated dataset to downstream consumers (Phase 2: Data Scientist role).

Implemented using:
	‚Ä¢	FastAPI

Endpoints:
	‚Ä¢	GET /books ‚Äì Paginated access to enriched books
	‚Ä¢	GET /books/{isbn} ‚Äì ISBN-based lookup
	‚Ä¢	GET /search ‚Äì Partial title/author search
	‚Ä¢	GET /sync ‚Äì Trigger pipeline execution without stopping the API

Design Principle:
Separation of concerns ‚Äî the API does not perform ingestion logic directly; it orchestrates it.

‚∏ª

Orchestration (main.py)

main.py acts as the control plane of the system:

Execution order:
	1.	Run OPAC sync
	2.	Identify new or updated records
	3.	Trigger incremental ingestion
	4.	Apply transformations
	5.	Update SQLite database

This ensures:
	‚Ä¢	Idempotency
	‚Ä¢	Recoverability
	‚Ä¢	Minimal redundant computation

‚∏ª

LLM Usage & Transparency

LLMs were used as assistive tools, not black boxes.

All interactions are logged in:

logs/project_log.md

Each entry records:
	‚Ä¢	Purpose of the prompt
	‚Ä¢	Tool used
	‚Ä¢	Summary of response
	‚Ä¢	How the output was applied

This satisfies the project‚Äôs LLM policy and ensures full transparency.

‚∏ª

How to Run

Install Dependencies

pip install -r requirements.txt

Run Full Pipeline

python main.py

Start API Server

python serving.py

Visit API docs at:

http://127.0.0.1:8000/docs


‚∏ª

Learning Outcomes

Through this project, we gained hands-on experience with:
	‚Ä¢	Designing end-to-end data pipelines
	‚Ä¢	Asynchronous ingestion at scale
	‚Ä¢	API rate-limit handling
	‚Ä¢	Schema design and relational storage
	‚Ä¢	Serving data for ML workloads

Most importantly, the project demonstrates how engineering decisions directly affect the usability of downstream machine learning systems.

‚∏ª

Future Work (Phase 2)
	‚Ä¢	Generate embeddings from book descriptions
	‚Ä¢	Implement semantic search
	‚Ä¢	Rank results based on query relevance
	‚Ä¢	Build a lightweight frontend

‚∏ª

Authors

DA-IICT ‚Äî Big Data Engineering Project