<<<<<<< HEAD
ðŸ“š Book Finder â€” Big Data Engineering Project

Overview

This project is part of the Big Data Engineering course at DA-IICT. The objective is to design and implement a robust data engineering pipeline that transforms sparse university library records into a high-quality, enriched dataset suitable for powering an ML-based semantic book search system.

The motivating use case is:

A user should be able to search for â€œa story about a lonely robot in spaceâ€ and retrieve relevant books, even if those exact words do not appear in the original library metadata.

This repository implements Phase 1 (Data Engineer role) of the overall project.

â¸»
=======
# ðŸ“š Book-Finder Pipeline

A robust, multi-stage data engineering pipeline designed to bridge the gap between physical library records and digital metadata. This project ingests accession registers, enriches them via the Google Books API, and serves the resulting data through a high-performance FastAPI service.

## ðŸ—ï¸ The "How & Why"

Library data is often fragmented. Our goal was to take raw accession registers (CSV/BibTeX) and transform them into a semantically rich database that could power a modern search application.

### Core Pipeline Logic
1.  **Sync (`sync_pipeline.py`)**: Crawls the Koha OPAC "New Arrivals" shelf. It handles the transition from physical to digital by fetching current shelf IDs and downloading BibTeX records.
2.  **Incremental Ingestion (`ingestion/ingestion.py`)**: Uses a high-concurrency `aiohttp` approach to fetch metadata from Google Books. It prioritizes *new* records by checking against a persistent JSONL log, preventing redundant API calls.
3.  **Transformation (`transformation/`)**: Resolves the "dirty data" problem. It merges raw CSVs with API results, handles complex Unicode characters, and performs strict deduplication based on Google IDs and ISBNs.
4.  **Serving (`api/serving.py`)**: A production-ready FastAPI layer that supports paginated listings, ISBN-based lookups, and real-time pipeline triggering.

## ðŸ›¡ï¸ Technical Challenges Overcome

Building this wasn't straightforward. We hit several significant hurdles:

-   **The "Altcha" Security Wall**: The university OPAC implemented a web-security check that occasionaly blocked automated BibTeX downloads. We implemented a bypass-aware sync that detects these blocks and falls back to existing records instead of corrupting the data.
-   **API Rate Limiting (HTTP 429)**: Google Books API is sensitive to concurrent requests. We solved this by implementing an **Asynchronous Semaphore** with exponential backoff, allowing us to process thousands of records without getting banned.
-   **Data Deduplication**: Merging library CSVs with Google data created a mess of overlapping records. We built a custom cleanup utility that prioritizes records with valid ISBNs and rich metadata, ensuring the final database is lean and accurate.
>>>>>>> 67eac781ebcd4f5da0b98c564eea6660b70a0f86

Problem Context & Motivation

<<<<<<< HEAD
University library datasets are typically operational, not analytical:
	â€¢	Titles and authors are present
	â€¢	Rich metadata (ISBNs, descriptions, subjects, thumbnails) is missing
	â€¢	Records are inconsistent and difficult to use for downstream ML tasks
=======
```bash
â”œâ”€â”€ ingestion/       # Async Google Books enrichment
â”œâ”€â”€ transformation/  # Data cleaning & deduplication
â”œâ”€â”€ storage/         # SQLite schema & loaders
â”œâ”€â”€ api/             # FastAPI service (serving.py)
â”œâ”€â”€ data/            # RAW, PROCESSED, and DB files
â”œâ”€â”€ sync_pipeline.py # OPAC Synchronization logic
â””â”€â”€ main.py          # Orchestration entry point
```
>>>>>>> 67eac781ebcd4f5da0b98c564eea6660b70a0f86

At DA-IICT, the library accession register CSV contains only:
	â€¢	Accession metadata
	â€¢	Title
	â€¢	Author / Editor
	â€¢	Publisher, Year, Pages

<<<<<<< HEAD
This project bridges that gap by:
	1.	Treating the DA-IICT CSV as the ground-truth scope
	2.	Enriching each record using external public APIs (Google Books)
	3.	Cleaning, deduplicating, and normalizing the data
	4.	Exposing the curated dataset via a FastAPI service for future ML use

â¸»

High-Level Architecture

The pipeline is designed as four explicit stages, mirroring real-world data engineering systems:
	1.	Ingestion â€“ Collect raw data from multiple sources
	2.	Transformation â€“ Clean, normalize, and deduplicate
	3.	Storage â€“ Persist curated data in a relational database
	4.	Serving â€“ Expose data through stable APIs

Each stage is implemented as a separate module, enabling clarity, testability, and extensibility.

â¸»

Data Sources

Primary Source (Scope Anchor)
	â€¢	DA-IICT Library Accession Register (CSV)
	â€¢	Defines the authoritative list of books
	â€¢	Ensures compliance with the DAU-only scope requirement

Secondary Source (Enrichment)
	â€¢	Google Books API
	â€¢	Used only to enrich existing DA-IICT titles
	â€¢	Provides:
	â€¢	ISBN-10 / ISBN-13
	â€¢	Descriptions / blurbs
	â€¢	Subjects / categories
	â€¢	Thumbnails and language metadata

No new books are introduced from APIsâ€”only metadata augmentation is performed.

â¸»

Detailed Pipeline Design

1. Sync Layer (sync_pipeline.py)

Purpose:
Handle synchronization with the libraryâ€™s OPAC system and manage incremental updates.

What it does:
	â€¢	Crawls the Koha OPAC â€œNew Arrivalsâ€ shelf
	â€¢	Dynamically identifies shelf IDs
	â€¢	Downloads BibTeX records where possible

Key Challenge:
	â€¢	The OPAC uses Altcha, a proof-of-work security mechanism that blocks automated scraping

Design Decision:
	â€¢	Detect security blocks gracefully
	â€¢	Fall back to existing records instead of corrupting the pipeline
	â€¢	Log warnings rather than failing hard

This ensures the pipeline remains fault-tolerant.

â¸»

2. Ingestion Layer (ingestion.py)

Purpose:
Fetch rich metadata for DA-IICT books using external APIs.

Key Design Choices:
	â€¢	Asynchronous requests using aiohttp
	â€¢	Concurrency control via asyncio.Semaphore
	â€¢	Exponential backoff with jitter to handle HTTP 429 rate limits
	â€¢	Incremental ingestion using JSONL logs to avoid re-fetching data

Why async?
	â€¢	The CSV contains ~26,000 records
	â€¢	Synchronous API calls would be prohibitively slow and rate-limited

Output:
	â€¢	Line-delimited JSON (.jsonl) files containing raw enriched metadata

â¸»

3. Transformation Layer (transformation.py)

Purpose:
Resolve real-world â€œdirty dataâ€ issues and prepare the dataset for storage.

Operations performed:
	â€¢	Unicode normalization (handling non-standard characters)
	â€¢	Title and author normalization
	â€¢	Removal of placeholder / empty descriptions
	â€¢	Deduplication using:
	â€¢	ISBN-13 (preferred)
	â€¢	Google Books ID (fallback)

Conflict Resolution Strategy:
	â€¢	Prefer records with valid ISBNs
	â€¢	Merge metadata instead of duplicating rows

This stage converts raw API responses into a clean, analytical dataset.

â¸»

4. Storage Layer (storage.py, db.py)

Purpose:
Persist curated data in a structured, queryable format.

Technology:
	â€¢	SQLite
	â€¢	SQLAlchemy ORM

Why SQLite?
	â€¢	Lightweight and portable
	â€¢	No external database dependency
	â€¢	Ideal for coursework and reproducibility

Schema Highlights:
	â€¢	books table
	â€¢	Indexed columns on isbn_13 and title
	â€¢	Enforced schema integrity

â¸»

5. Serving Layer (serving.py)

Purpose:
Expose the curated dataset to downstream consumers (Phase 2: Data Scientist role).

Implemented using:
	â€¢	FastAPI

Endpoints:
	â€¢	GET /books â€“ Paginated access to enriched books
	â€¢	GET /books/{isbn} â€“ ISBN-based lookup
	â€¢	GET /search â€“ Partial title/author search
	â€¢	GET /sync â€“ Trigger pipeline execution without stopping the API

Design Principle:
Separation of concerns â€” the API does not perform ingestion logic directly; it orchestrates it.

â¸»

Orchestration (main.py)

main.py acts as the control plane of the system:

Execution order:
	1.	Run OPAC sync
	2.	Identify new or updated records
	3.	Trigger incremental ingestion
	4.	Apply transformations
	5.	Update SQLite database

This ensures:
	â€¢	Idempotency
	â€¢	Recoverability
	â€¢	Minimal redundant computation

â¸»

LLM Usage & Transparency

LLMs were used as assistive tools, not black boxes.

All interactions are logged in:

logs/project_log.md

Each entry records:
	â€¢	Purpose of the prompt
	â€¢	Tool used
	â€¢	Summary of response
	â€¢	How the output was applied

This satisfies the projectâ€™s LLM policy and ensures full transparency.

â¸»

How to Run

Install Dependencies

pip install -r requirements.txt

Run Full Pipeline

python main.py

Start API Server

python serving.py

Visit API docs at:

http://127.0.0.1:8000/docs


â¸»

Learning Outcomes

Through this project, we gained hands-on experience with:
	â€¢	Designing end-to-end data pipelines
	â€¢	Asynchronous ingestion at scale
	â€¢	API rate-limit handling
	â€¢	Schema design and relational storage
	â€¢	Serving data for ML workloads

Most importantly, the project demonstrates how engineering decisions directly affect the usability of downstream machine learning systems.

â¸»

Future Work (Phase 2)
	â€¢	Generate embeddings from book descriptions
	â€¢	Implement semantic search
	â€¢	Rank results based on query relevance
	â€¢	Build a lightweight frontend

â¸»

Authors

DA-IICT â€” Big Data Engineering Project
=======
1. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Run Full Pipeline**:
   ```bash
   python main.py
   ```
   *Note: This will automatically sync, ingest new items, transform, and update the DB.*

3. **Start API Server**:
   ```bash
   python api/serving.py
   ```
   *Access the docs at: http://127.0.0.1:8000/docs*
>>>>>>> 67eac781ebcd4f5da0b98c564eea6660b70a0f86
